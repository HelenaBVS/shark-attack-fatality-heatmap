{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Shark Attack Incidents\n",
    "## Heatmap\n",
    "\n",
    "This notebook leverages a dataset containing records of shark attack incidents worldwide to explore and visualize the frequency of these occurrences. By analyzing this data, we gain insights into the geographic distribution of shark attacks, shedding light on areas where such incidents are more prevalent. Towards the end of the notebook, a heat map is presented, providing a visual representation of the regions most affected by shark attacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shark data (import and visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/helena/anaconda3/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/helena/anaconda3/lib/python3.11/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/helena/anaconda3/lib/python3.11/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/helena/anaconda3/lib/python3.11/site-packages (from seaborn) (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/helena/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/helena/anaconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/helena/anaconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/helena/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o4rUGPnntPvX"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/helena/Documents/PUCRS_files/Curso - CDIA/Intro a CD/global-sharkdataset-GSAF5.xls.csv/GSAF5.xls.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     15\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mThe low_memory=False parameter is optional and was set to False.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mWhen set to False, it instructs pandas not to attempt to infer the data type of each column automatically,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mwe're reading the dataset.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/helena/Documents/PUCRS_files/Curso - CDIA/Intro a CD/global-sharkdataset-GSAF5.xls.csv/GSAF5.xls.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCase Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArea\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 9\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecies \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFatal (Y/N)\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInjury\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/helena/Documents/PUCRS_files/Curso - CDIA/Intro a CD/global-sharkdataset-GSAF5.xls.csv/GSAF5.xls.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\"\"\" \n",
    "Setting display.max_columns to None ensures that Pandas will display all columns of a DataFrame, \n",
    "regardless of its size. This can be useful when working with datasets that have a large number of columns, \n",
    "as it allows you to see all the available data without truncation.\n",
    "It improves the readability and usability of the DataFrame, especially when exploring or analyzing \n",
    "the data interactively.\n",
    "\"\"\"\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\"\"\"\n",
    "The low_memory=False parameter is optional and was set to False.\n",
    "When set to False, it instructs pandas not to attempt to infer the data type of each column automatically,\n",
    "which can save memory in cases where the file has many columns with mixed data types or large amounts of data.\n",
    "This can be useful when working with large files or varied data types.\n",
    "However, setting low_memory=False may consume more RAM during the file reading process,\n",
    "we're reading the dataset.\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"./inputs/sharks/GSAF5.xls.csv\", low_memory=False)\n",
    "print(df.columns)\n",
    "\n",
    "df = df[['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area',\n",
    "       'Location', 'Activity', 'Name', 'Unnamed: 9', 'Age', 'Time','Species ','Fatal (Y/N)','Injury']]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "It's not strictly necessary to maintain the original order of columns when renaming them. \n",
    "However, doing so can enhance clarity, consistency, and compatibility with existing code.\n",
    "\"\"\"\n",
    "# Renaming column ('Unnamed: 9' to 'Victim\\'s Gender')\n",
    "df.columns = ['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area',\n",
    "       'Location', 'Activity', 'Name', 'Victim\\'s Gender', 'Age', 'Time','Species','Fatal', 'Injury']\n",
    "\n",
    "# Amount of \"(lines, columns)\" respectively\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df.isna() == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is using the .isnull () method of the DataFrame to check null values in each column\n",
    "and then it is adding the number of null values in each of these columns, to obtain its total.\n",
    "\"\"\"\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The nunique() function in Pandas is used to count the number of unique values in a specific column of a DataFrame.\n",
    "Applying this function to the 'Country' column provides valuable insight into the shark incidents,\n",
    "highlighting the global nature of shark attacks documented in the dataset. \n",
    "This information is crucial for understanding the geographical distribution and scope of the dataset.\n",
    "\"\"\"\n",
    "df['Country'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the occurrences of shark attacks by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The \"value_counts()\" function shows the occurrences of shark attacks by country.\n",
    "The \"reset_index()\" method converts the results into a DataFrame.\n",
    "Finally, \"head(20)\", wich is also a method exhibits the top twenty countries with the highest\n",
    "number of occurrences, offering a quick view of the most affected countries by shark attacks,\n",
    "Brazil (the country where I live) is in the seventh position.\n",
    "\"\"\"\n",
    "# Selecting the column 'Country' from the DataFrame and adding the function `value_counts()`\n",
    "# in order to create the 'count' column, so that we can find out the number of occurrences, respectively.\n",
    "df_country = df['Country'].value_counts()\n",
    "\n",
    "# Counting the occurrences of shark attacks by country and storing the result in a DataFrame\n",
    "df_country = df['Country'].value_counts().reset_index()\n",
    "\n",
    "# Displaying the top 20 countries with the highest number of shark attack occurrences\n",
    "df_country.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DILKkdNC94fT"
   },
   "outputs": [],
   "source": [
    "# Plotting a bar graph that shows the 20 countries with the most occurrences of shark attacks\n",
    "df_country.head(20).plot.bar('Country')\n",
    "\n",
    "# Adding a title to the graph\n",
    "plt.title(\"Number of Shark Attacks\")\n",
    "\n",
    "# Displaying the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a percentage of the total, USA is 36% and Australia is 21%, so these two countries account for half of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the rate of shark attack occurrences for each country relative to the total occurrences worldwide\n",
    "# by dividing the number of shark attack occurrences for each country by the total occurrences of all countries.\n",
    "# Add a new column 'rate' to the DataFrame df_country to store these rates.\n",
    "df_country['rate'] = df_country.iloc[:,1] / df_country['count'].sum()\n",
    "\n",
    "# Display the first five rows of the DataFrame df_country, including the new 'rate' column.\n",
    "df_country.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The count of shark attacks is stored in the column 'Type'.\n",
    "# The reset_index() method is used to reset the index of the resulting DataFrame, ensuring that \n",
    "# the row labels are sequential integers, since that facilitates the access to specific rows and enhances\n",
    "# compatibility with various data manipulation operations in many libraries. \n",
    "df_Area = df[['Country','Area','Type']].groupby(['Country','Area']).count().reset_index().sort_values('Type',ascending=False)\n",
    "\n",
    "df_Area.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocation Data (Importing and Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the dataset [Geolocation Data](https://www.kaggle.com/datasets/liewyousheng/geolocation/data)\n",
    "# and reading the 'inputs/geolocation/cities.csv' file in order to obtain a better parameter in geolocation\n",
    "# and cross with the data from the Shark dataset.\n",
    "place_data = pd.read_csv('./inputs/geolocation/cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the columns 'state_name', 'country_name', 'latitude', and 'longitude' from the DataFrame place_data\n",
    "# and storing the result back into the variable place_data.\n",
    "place_data = place_data[['state_name','country_name','latitude','longitude']]\n",
    "\n",
    "# Cleaning the data by removing duplicate rows based on the combination of 'state_name' and 'country_name'\n",
    "# subset parameter specifies the columns to consider when identifying duplicates.\n",
    "# In this case, we want to drop rows where both 'state_name' and 'country_name' are the same.\n",
    "place_data = place_data.drop_duplicates(subset=['state_name','country_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulating string data in the 'state_name' column of the 'place_data' DataFrame:\n",
    "# 1. Converting all letters in the 'state_name' column to lowercase using the str.lower() method.\n",
    "place_data['state_name'] = place_data['state_name'].str.lower()\n",
    "\n",
    "# 2. Removing all whitespace characters (' ') from the 'state_name' column using the str.replace() method.\n",
    "# This step ensures consistency in string formatting and removes unnecessary whitespace.\n",
    "place_data['state_name'] = place_data['state_name'].str.replace(' ', '')\n",
    "\n",
    "place_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The shape attribute of a DataFrame or a NumPy array returns a tuple representing the dimensions of the \n",
    "data structure. In the specific case of place_data.shape, it returns \n",
    "a tuple with two elements: the first element represents the number of rows (or entries) \n",
    "in the DataFrame or array, and the second element represents the number of columns (or features) \n",
    "in the DataFrame or array. Therefore, when calling place_data.shape, you will obtain the \n",
    "total number of entries (or records) and the total number of features (or variables) in the DataFrame place_data. \n",
    "This is useful for understanding the data structure and determining its dimensions \n",
    "before performing additional operations or analyses.\n",
    "\"\"\"\n",
    "place_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing only the 'Country' and 'Area' columns from the original DataFrame\n",
    "df_only_Area = df[['Country', 'Area']]\n",
    "\n",
    "# Rename the 'Area' column to 'state_name' in order to reflect its content more accurately\n",
    "df_only_Area.columns = ['Country', 'state_name']\n",
    "\n",
    "# Convert the values in the 'state_name' column to lowercase\n",
    "df_only_Area['state_name'] = df_only_Area['state_name'].str.lower()\n",
    "\n",
    "# Remove whitespace characters from the values in the 'state_name' column\n",
    "df_only_Area['state_name'] = df_only_Area['state_name'].str.replace(' ', '')\n",
    "\n",
    "# Display the first few rows of the preprocessed DataFrame\n",
    "df_only_Area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_Area.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join operation to merge the DataFrame df_only_Area (left) with the place_data DataFrame (right) \n",
    "# on the 'state_name' column. This operation is useful when we want to retain all the data from the left DataFrame \n",
    "# (df_only_Area) and only add additional information from the right DataFrame (place_data) where available.\n",
    "df_join = pd.merge(df_only_Area, place_data, how='left', on='state_name')\n",
    "\n",
    "# Display the shape of the resulting DataFrame to check the number of rows and columns\n",
    "print(df_join.shape)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "df_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the two sets of data have different purposes, there are null data that have not been combined well.\n",
    "In this case, all nulls will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the merged DataFrame df_join\n",
    "df_join.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows where the 'country_name' column is null (missing)\n",
    "df_join[pd.isnull(df_join.country_name)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Drop rows with missing values and specific columns from the DataFrame df_join.\n",
    "# The axis=0 parameter indicates that rows will be dropped.\n",
    "# The .drop() function is used to remove rows or columns from a DataFrame.\n",
    "# Here, we remove rows containing missing values using dropna(), and specific columns \n",
    "# ('Country', 'country_name', 'state_name') using the 'axis=1' parameter.\n",
    "\"\"\"\n",
    "df_join = df_join.dropna(axis=0).drop(['Country','country_name','state_name'], axis=1)\n",
    "\n",
    "# Check for missing values after dropping rows \n",
    "print(df_join.isnull().sum())\n",
    "\n",
    "# Display the shape of the DataFrame after dropping rows and columns\n",
    "df_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This displays the first few rows of the DataFrame, allowing the data to be inspected \n",
    "# in order to confirm that the preprocessing steps have been applied correctly.\n",
    "df_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command instaled the Folium package, which is used for visualizing geospatial data with interactive maps. \n",
    "# Once installed, I was able to import and use Folium in the Jupyter notebook.\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame values to a list format\n",
    "df_list = df_join.values.tolist()\n",
    "\n",
    "# Import the Folium library for creating interactive maps\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map centered at latitude 10 and longitude -20, with a zoom level of 3\n",
    "map = folium.Map(location=[10, -20], zoom_start=3)\n",
    "\n",
    "# Create a HeatMap layer using the list of values converted from the DataFrame\n",
    "# with a radius of 7 and a blur factor of 5, and add it to the map\n",
    "HeatMap(df_list, radius=7, blur=5).add_to(map)\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully visualized heatmap!\n",
    "\n",
    "We can see that there are incidents with sharks in various locations in the United States. However, it's important to note that the latitude and longitude data may not be perfectly accurate, resulting in some coloration even on land due to imprecise geolocation data. Nevertheless, exploring this heatmap can still provide valuable insights into areas where shark incidents are more prevalent.\n",
    "\n",
    "Feel free to interact with the map by moving around the region and zooming in to explore specific areas. This visualization can be useful for gaining awareness and understanding potential risk areas, allowing you to make informed decisions and take precautions when traveling or engaging in activities near coastal areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#random florest (the best option?)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./inputs/sharks/GSAF5.xls.csv\",low_memory=False)\n",
    "print(df.info)\n",
    "\n",
    "df = df[['Case Number','Time', 'Date', 'Year', 'Type', 'Country','Fatal','Injury', 'Area','Location', 'Activity', 'Name', 'Victim\\'s Gender', 'Age','Species']]\n",
    "#df_ml = df[['Time','Country','Location','Fatal','Injury','Age','Species',Activity','Victim\\'s Gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "# Check NaNs\n",
    "print(\"\\nNumber of NaNs in each column:\\n\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in critical columns\n",
    "critical_columns = ['Country', 'Location', 'Activity', 'Injury']\n",
    "missing_data_percentage = df[critical_columns].isna().mean() * 100\n",
    "print(\"Percentage of missing data in critical columns:\\n\", missing_data_percentage)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap to visualize missing data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df[critical_columns].isna(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Data in Critical Columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Strategy for Shark Attack Dataset\n",
    "\n",
    "Based on the analysis of missing values (NaNs) in the shark attack dataset, we need a more nuanced approach to clean the data while retaining as much valuable information as possible.\n",
    "\n",
    "## NaN Analysis Summary\n",
    "\n",
    "The following columns have been identified with the respective number of NaNs:\n",
    "\n",
    "- **Case Number**: 2 NaNs\n",
    "- **Date**: 1 NaN\n",
    "- **Year**: 3 NaNs\n",
    "- **Type**: 5 NaNs\n",
    "- **Country**: 51 NaNs\n",
    "- **Area**: 463 NaNs\n",
    "- **Location**: 545 NaNs\n",
    "- **Activity**: 552 NaNs\n",
    "- **Name**: 215 NaNs\n",
    "- **Victim's Gender**: 6434 NaNs\n",
    "- **Age**: 2871 NaNs\n",
    "- **Time**: 3392 NaNs\n",
    "- **Species**: 2924 NaNs\n",
    "- **Fatal**: 547 NaNs\n",
    "- **Injury**: 29 NaNs\n",
    "\n",
    "## Data Cleaning Strategy\n",
    "\n",
    "### Columns to Drop NaNs\n",
    "\n",
    "For columns where a minimal number of entries are missing, we will drop the rows containing NaNs. This is crucial for columns where missing values would significantly impact the analysis.\n",
    "\n",
    "- **Country**: 51 NaNs\n",
    "- **Location**: 545 NaNs\n",
    "- **Activity**: 552 NaNs\n",
    "- **Injury**: 29 NaNs\n",
    "\n",
    "### Columns to Fill NaNs\n",
    "\n",
    "For columns with a high percentage of missing values, we will fill or impute the NaNs with appropriate values to retain as much data as possible.\n",
    "\n",
    "- **Age**: 2871 NaNs (Fill with median)\n",
    "- **Time**: 3392 NaNs (Fill with 'Unknown')\n",
    "- **Victim's Gender**: 6434 NaNs (Fill with 'Unknown')\n",
    "- **Species**: 2924 NaNs (Fill with 'Unknown')\n",
    "\n",
    "### Other Columns with Minimal NaNs\n",
    "\n",
    "For columns with a minimal number of missing values, we will handle these individually by dropping rows with NaNs.\n",
    "\n",
    "- **Case Number**\n",
    "- **Date**\n",
    "- **Year**\n",
    "- **Type**\n",
    "- **Fatal**\n",
    "- **Area**\n",
    "- **Name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing / Nan\n",
    "\n",
    "! Further Cleaning !\n",
    "\n",
    "4FUTURE: Review entries with 'Unknown' values and investigate if additional data sources or methods can reduce these unknowns.\n",
    "Consider imputing values based on more sophisticated methods if further cleaning is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gender_guesser.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(case_sensitive=False)\n",
    "\n",
    "# Filled with 'Unknown' to avoid dropping rows, ensuring the dataset remains comprehensive.\n",
    "# 4FUTURE: Find a library (or similar) to help w/ that - We need to read the gender\n",
    "#df['Victim\\'s Gender'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Define a function to guess gender from name\n",
    "def guess_gender(name):\n",
    "    if pd.isnull(name) or name.strip() == '':\n",
    "        return 'Unknown'\n",
    "\n",
    "    if name == 'male':\n",
    "        return 'M'\n",
    "    elif name == 'female':\n",
    "        return 'F'\n",
    "    \n",
    "    gender = detector.get_gender(name.split()[0]) \n",
    "    if gender in ['male', 'mostly_male']:\n",
    "        return 'M'\n",
    "    elif gender in ['mostly_female', 'female']:\n",
    "        return 'F'\n",
    "            \n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define critical columns\n",
    "critical_columns = ['Country', 'Location', 'Activity', 'Injury']\n",
    "\n",
    "# Drop rows with NaNs in critical columns if percentage is acceptable\n",
    "df = df.dropna(subset=critical_columns)\n",
    "\n",
    "# Filled with 'Unknown' to maintain consistency and handle missing values without assuming incorrect times.\n",
    "# 4FUTURE: Considered doing the MEDIAN \n",
    "df['Time'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Filled with 'Unknown' to retain entries even if the species is not identified.\n",
    "# 4FUTURE: Find a library (or similar) to help w/ that - this variable it's helpfull to read Injury (and only that. ?)\n",
    "df['Species'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Filled with 'Unknown' to retain information on non-fatal incidents.\n",
    "# 4FUTURE: Reading Injury this should be easier to read\n",
    "df['Fatal'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Drop remaining rows with NaNs in minimal NaN columns\n",
    "df = df.dropna(subset=['Case Number', 'Date', 'Year', 'Type', 'Fatal', 'Area', 'Name'])\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nNumber of NaNs after treatment:\\n\\n\", df.isna().sum())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Remove irrelevant columns\n",
    "#df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "\n",
    "# Check NaNs\n",
    "#print(\"\\nNumber of NaNs before treatment:\\n\\n\", df.isna().sum())\n",
    "\n",
    "# Handle NaNs\n",
    "#df = df.dropna(subset=['Time','Country','Location','Fatal','Injury','Age','Activity','Species','Victim\\'s Gender'])\n",
    "\n",
    "# Check again\n",
    "#print(\"\\nNumber of NaNs after treatment:\\n\\n\", df.isna().sum())\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in 'Age' column\n",
    "print(df['Age'].unique())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"].replace(['30s', '60s', \"20's\", '40s', 'a minor', '20s', 'Teen', \n",
    "                               '18 months', '50s', 'teen', '6½', 'mid-30s', '20?', \"60's\", \n",
    "                               'Elderly', 'mid-20s', 'Ca. 33', '>50', 'adult', '9 months', \n",
    "                               '(adult)', 'X', '\"middle-age\"', '2 to 3 months', \n",
    "                               'MAKE LINE GREEN', '\"young\"', 'F', 'young', 'A.M.', \n",
    "                               '2½'], \n",
    "                                ['35', '65', '25', '45', '16', '25', '14', '1', \n",
    "                                '55', '14', '6', '35', '20', '65', '60', '25', \n",
    "                                '33', '55', '30', '0', '30', 'unkown', '50', '0', \n",
    "                                'unkown', '20', 'unkown', '20', 'unkown', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs in other columns with appropriate values\n",
    "# Converted to numeric and filled with the median age, Retaining the central tendency of the data.\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Shark Incidents by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Age Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Age'], bins=30, kde=True, color='blue')\n",
    "plt.title('Distribution of Shark Incidents by Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently it is possible to visualize shark encounters, not shark attacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which period people are attacked the most? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ´Time´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize time\n",
    "def categorize_time(time):\n",
    "    if pd.isnull(time) or time.strip() == '':\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    time = time.strip().lower()\n",
    "    \n",
    "    # Direct categories\n",
    "    if \"morning\" in time:\n",
    "        return \"Morning\"\n",
    "    elif \"afternoon\" in time or \"pm\" in time:\n",
    "        return \"Afternoon\"\n",
    "    elif \"evening\" in time or \"night\" in time:\n",
    "        return \"Evening\"\n",
    "    elif \"early morning\" in time:\n",
    "        return \"Morning\"\n",
    "    elif \"late afternoon\" in time:\n",
    "        return \"Afternoon\"\n",
    "    \n",
    "    # Specific time conversion\n",
    "    try:\n",
    "        time = time.replace('h', ':').replace('H', ':')\n",
    "        hour = int(time.split(':')[0])\n",
    "        if 0 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif 12 <= hour < 18:\n",
    "            return \"Afternoon\"\n",
    "        else:\n",
    "            return \"Evening\"\n",
    "    except ValueError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply the categorization function\n",
    "df['Time'] = df['Time'].apply(categorize_time)\n",
    "\n",
    "# Fill remaining missing values with 'Unknown'\n",
    "df['Time'] = df['Time'].fillna('Unknown')\n",
    "print(df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all entries in the 'Time' column\n",
    "print(\"\\nCount of all values in the 'Time' column:\")\n",
    "print(df['Time'])\n",
    "\n",
    "# Filter and visualize only the entries that are not 'Unknown' - checking the cleaning\n",
    "non_unknown_times = df[df['Time'] == 'Unknown']\n",
    "print(\"\\nEntries in the 'Time' column that are not 'Unknown':\")\n",
    "print(non_unknown_times['Time'].value_counts())\n",
    "print(non_unknown_times[['Time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart to visualize the distribution of shark attacks over time\n",
    "time_counts = df['Time'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_counts.plot(kind='bar', color=['violet','crimson','gold','lightgreen'])\n",
    "plt.title('Distribution of Shark Incidents by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do Shark Attacks Occur More in the Afternoon?\n",
    "\n",
    "We observe that shark attacks are more common in the afternoon due to a blend of human activity, shark behavior, and environmental factors.\n",
    "\n",
    "## Human Activity\n",
    "- **Peak Beach Hours**: In the afternoon, there is a significant increase in beachgoers participating in swimming, surfing, and other water activities. This heightened human presence in the water elevates the likelihood of encounters with sharks.\n",
    "- **Surfing Patterns**: Surfers, who are frequently attacked, often prefer the afternoon for better wave conditions, leading them to spend more time in the water and becoming more susceptible to shark encounters.\n",
    "\n",
    "## Shark Behavior\n",
    "- **Feeding Times**: Sharks are generally more active in their feeding during dawn and dusk, coinciding with peak human activity in the water during these times.\n",
    "- **Prey Movement**: The movement of prey fish closer to shore in the afternoon draws sharks nearer to coastal areas, increasing the chances of shark-human interactions.\n",
    "\n",
    "## Environmental Factors\n",
    "- **Water Conditions**: Warmer and clearer water conditions in the afternoon attract more marine life, including sharks, thus heightening the potential for shark attacks.\n",
    "\n",
    "Understanding these patterns can help us develop strategies to reduce the risk of shark attacks by avoiding peak shark activity times and raising awareness among beachgoers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher incidence of shark attacks (M/F)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4FUTURE: Kaggle notebooks has restrictions -- Figured it out\n",
    "!pip install gender-guesser\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gender values to readable format\n",
    "df['Victim\\'s Gender'] = df['Victim\\'s Gender'].map({'F': 'Female', 'M': 'Male'})\n",
    "\n",
    "print(df['Victim\\'s Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gender_guesser.detector import Detector\n",
    "\n",
    "# Apply the function to the 'Name' column\n",
    "df['Predicted Gender'] = df['Name'].apply(guess_gender)\n",
    "\n",
    "# Replace 'Unknown' values in 'Victim\\'s Gender' with predicted values where possible\n",
    "#df['Victim\\'s Gender'] = df.apply(lambda row: row['Victim\\'s Gender'] if row['Victim\\'s Gender'] != 'NaN' else row['Predicted Gender'], axis=1)\n",
    "\n",
    "# Drop the 'Predicted Gender' column as it's no longer needed\n",
    "#df.drop(columns=['Predicted Gender'], inplace=True)\n",
    "\n",
    "print(df['Predicted Gender'].value_counts())\n",
    "\n",
    "# Remove 'Unknown's\n",
    "#df = df[df['Victim\\'s Gender'] != 'Unknown']\n",
    "print(df['Predicted Gender'].value_counts())\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows where 'Victim\\'s Gender' has NaN values\n",
    "nan_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "# Save rows with NaN values to a CSV file for manual review\n",
    "nan_rows.to_csv('rows_with_nans.csv', index=False)\n",
    "print(\"Rows with NaN values saved to 'rows_with_nans.csv' for manual review.\")\n",
    "\n",
    "# Create a link to download the CSV file\n",
    "from IPython.display import FileLink\n",
    "FileLink('rows_with_nans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['age'] != 'unknown']\n",
    "\n",
    "\n",
    "# Count the number of incidents by gender\n",
    "gender_counts = df['Predicted Gender'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of shark incidents by gender:\")\n",
    "print(gender_counts)\n",
    "\n",
    "# Visualize the distribution of incidents by gender\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "gender_counts.plot(kind='bar', color=['violet','lightblue', 'lightpink'])\n",
    "plt.title('Distribution of Shark Incidents by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Understanding the Activity variable\n",
    "df['Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to automate the categorization of activities\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a PhraseMatcher object\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "categories = {\n",
    "    \"Surface Water Sports\": [\"surfing\", \"stand-up paddleboarding\", \"kayak fishing\", \"windsurfing\", \"kite surfing\", \"body boarding\"],\n",
    "    \"Underwater Activities\": [\"scuba diving\", \"snorkeling\", \"spearfishing\", \"free diving\"],\n",
    "    \"Casual Water Activities\": [\"swimming\", \"floating\", \"wading\", \"bathing\"],\n",
    "    \"Fishing and Hunting\": [\"fishing\", \"spearfishing\", \"crabbing\", \"lobstering\", \"collecting seashells\"],\n",
    "    \"Interaction with Marine Life\": [\"feeding sharks\", \"touching a shark\", \"photographing marine life\"],\n",
    "    \"Unique or Unspecified\": [\"sea disaster\", \"walking on the beach\", \"cleaning fish\", \"standing in water\"]\n",
    "}\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for category, activities in categories.items():\n",
    "    patterns = [nlp.make_doc(text) for text in activities]\n",
    "    matcher.add(category, patterns)\n",
    "\n",
    "# Function to process text and extract features or categorize\n",
    "def categorize_activities(activity):\n",
    "    try:\n",
    "        doc = nlp(activity)\n",
    "        matches = matcher(doc)\n",
    "        \n",
    "        main_match = matches[0]\n",
    "        match_id = main_match[0]\n",
    "        rule_id = nlp.vocab.strings[match_id]\n",
    "\n",
    "        return rule_id\n",
    "    \n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply the function to the 'Activity' column\n",
    "df['Activity (Categorized)'] = df['Activity']\n",
    "df['Activity (Categorized)'] = df['Activity (Categorized)'].apply(categorize_activities)\n",
    "\n",
    "activity_counts = df['Activity (Categorized)'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Move \"Unknown\" to the end\n",
    "if 'Unknown' in activity_counts:\n",
    "    unknown_count = activity_counts['Unknown']\n",
    "    activity_counts = activity_counts.drop('Unknown')\n",
    "    activity_counts['Unknown'] = unknown_count\n",
    "\n",
    "print(activity_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "activity_counts.plot(kind='bar', color=['crimson', 'gold', 'lightgreen', 'violet', 'brown', 'orange', 'blue'])\n",
    "plt.title('Distribution of Shark Incidents by Victim\\'s Activity at the moment')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if it was an attack or an encounter\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "\n",
    "# Initialize the SpaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the classification component to the pipeline\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "\n",
    "# Add labels to the classification component\n",
    "textcat.add_label(\"attack\")\n",
    "textcat.add_label(\"encounter\")\n",
    "\n",
    "# 4FUTURE: train the model with more parameters (for example, FATAL is giving Encounter)\n",
    "# Manually annotated dataset (example)\n",
    "train_data = [\n",
    "    (\"severe lacerations to left forearm\", {\"cats\": {\"attack\": 1, \"encounter\": 0}}),\n",
    "    (\"no injury, shark bit board\", {\"cats\": {\"attack\": 0, \"encounter\": 1}}),\n",
    "    (\"lacerations to right foot\", {\"cats\": {\"attack\": 1, \"encounter\": 0}}),\n",
    "    (\"no injury, kayak bitten\", {\"cats\": {\"attack\": 0, \"encounter\": 1}}),\n",
    "    (\"fatal\", {\"cats\": {\"attack\": 1, \"encounter\": 0}})\n",
    "]\n",
    "\n",
    "# Create DocBin for storing training data\n",
    "doc_bin = DocBin()\n",
    "\n",
    "for text, annotations in train_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    doc_bin.add(example.reference)\n",
    "\n",
    "# Model training\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "for i in range(10):  # Number of training epochs\n",
    "    losses = {}\n",
    "    batches = spacy.util.minibatch(train_data, size=2)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
    "        nlp.update(examples, drop=0.5, losses=losses)\n",
    "\n",
    "# Testing the model with new descriptions\n",
    "texts = [\"puncture wounds to leff foot & lower leg\", \"no injury, board bitten\", \"injuries to right leg & hand\"]\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "# Using the model on dataset data\n",
    "# Define the function that will categorize the attacks based on the recorded Injury\n",
    "def categorize_injury(injury):\n",
    "    doc = nlp(injury)\n",
    "    \n",
    "    if doc.cats[\"attack\"] > doc.cats[\"encounter\"]:\n",
    "        return \"Attack\"\n",
    "    \n",
    "    else:\n",
    "        return \"Encounter\"\n",
    "    \n",
    "# Apply the categorization function\n",
    "df[\"Attack or Encounter\"] = df[\"Injury\"].apply(categorize_injury)\n",
    "df[[\"Injury\", \"Attack or Encounter\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Include species data in the injury categorization\n",
    "#def categorize_injury(injury, species):\n",
    "    #doc = nlp(injury)\n",
    "    \n",
    "    # Check if the species indicates a non-shark\n",
    "    #if \"not a shark\" in species.lower():\n",
    "        #return \"Encounter\"\n",
    "    \n",
    "    #if doc.cats[\"attack\"] > doc.cats[\"encounter\"]:\n",
    "        #return \"Attack\"\n",
    "    \n",
    "    #else:\n",
    "        #return \"Encounter\"\n",
    "\n",
    "# Apply the categorization function with species consideration\n",
    "#df[\"Attack or Encounter\"] = df.apply(lambda row: categorize_injury(row['Injury'], row['Species']), axis=1)\n",
    "#df[[\"Injury\", \"Species\", \"Attack or Encounter\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1991032,
     "sourceId": 3288213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4109592,
     "sourceId": 7124238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
